---
title: "Encodings and cross-validation"
author: "Michal Burdukiewicz, Piotr Sobczyk"
output: html_document
bibliography: amyloids.bib
---

```{r, echo = FALSE, message = FALSE, results='asis'}
source("./functions/report_functions.R")
source("./functions/plot_tools.R")
```

Date: `r Sys.Date()`

# Encodings of amino acids

The amylodogenicity of given peptide may not depend on the exact sequence of amino acids, but on its more general properties. To verify this hypothesis, we created 18 537 reduced amino acid alphabets with different lengths (from three to six letters).

## Physicochemical properties

We created the reduced alphabet of amino acids using Ward's clusterization on the selected physicochemical properties. We picked several measures belonging to more general categories important in process of amylodogenicity as size, hydrophobicity, solvent surface area, frequency in $\beta$-sheets and contactivity. As the rule of thumb, we limited ourselves to properties introduced after 1980 when, thanks to the technological advancements, the measurements were more accurate.

We further reduced the number of properties to 17, by selecting measures uncorrelated with others (with the Pearson's correlation coefficient for normalized values larger than 0.95 or smaller than 0.05) since they would create very similar encodings.

```{r, echo = FALSE, message = FALSE, results='asis'}
choose_properties() %>%
  format_property_table %>%
  kable(format = "pandoc")
```

## Encoding distance

The encoding distance between **a** and **b** is defined as the minimum number of amino acids that have to be moved between subgroups of encoding to make **a** identical to **b** (the order of subgroups in the encoding and amino acids in a group is unimportant).

The encoding distance may be further scaled by a factor reflecting how much moving amino acids between groups altered mean group properties.

```{r, echo = FALSE, message = FALSE, results='asis'}
a <- structure(list(`1` = "p", `2` = c("f", "i", "w", "y"), 
                    `3` = c("a", "c", "d", "e", "g", "h", "k", "l", "m", "n", "q", "r", "s", "t", "v")), 
               .Names = c("1", "2", "3"))

kable(group2df(a), caption = "Encoding A")
```


```{r, echo = FALSE, message = FALSE, results='asis'}
b <- structure(list(`1` = c("f", "r", "w", "y"), `2` = c("c", "i", "l", "t", "v"), 
                    `3` = c("a", "d", "e", "g", "h", "k", "m", "n", "p", "q", "s")), 
               .Names = c("1", "2", "3"))

kable(group2df(b), caption = "Encoding B")
```

The encoding distance between both groups is `r calc_ed(a, b)`.  

```{r, echo = FALSE, message = FALSE, results='asis'}
a_prop <- structure(c(0.112469437652812, 0.405737704918033, 0.711491442542787, 
                      0.332991803278688, 0.327628361858191, 0.156762295081967, 0.256723716381418, 
                      0.00614754098360652, 0.312958435207824, 0.905737704918033, 0.440097799511002, 
                      0.145491803278689, 0.36919315403423, 0.055327868852459, 0, 0.262295081967213, 
                      0.562347188264059, 0.559426229508197, 0.454767726161369, 1, 0.454767726161369, 
                      0.941598360655738, 0.535452322738386, 0, 0.540342298288509, 0.787909836065574, 
                      0.709046454767726, 0.968237704918033, 0.320293398533007, 0.117827868852459, 
                      0.15158924205379, 0.137295081967213, 0.264058679706601, 0.305327868852459, 
                      1, 0.961065573770492, 0.728606356968215, 0.648565573770492, 0.342298288508558, 
                      0.88422131147541), .Dim = c(2L, 20L), 
                    .Dimnames = list(c("CHAM820101", "NISK860101"), 
                                     c("A", "R", "N", "D", "C", "Q", "E", "G", "H", "I", "L", "K", "M", "F", "P", "S", "T", "W", "Y", "V")))

colnames(a_prop) <- tolower(colnames(a_prop))

#b_prop <- aa_nprop[na.omit(traits_table[ao, ]), , drop = FALSE]

# must have unified lists of features

coords_a <- lapply(a, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))
coords_b <- lapply(b, function(single_subgroup) rowMeans(a_prop[, single_subgroup, drop = FALSE]))

dat_a <- data.frame(enc = "a", do.call(rbind, coords_a), label = paste0("A", 1L:3))
dat_b <- data.frame(enc = "b", do.call(rbind, coords_b), label = paste0("B", 1L:3))

dat <- data.frame(do.call(rbind, lapply(1L:nrow(dat_a), function(id) 
  data.frame(id = id, rbind(do.call(rbind, lapply(1L:3, function(dummy) 
    dat_a[id, , drop = FALSE])),
    dat_b)))), pair = c(paste0("d", 1L:3), paste0("d", 1L:3)))

colnames(dat) <- c("id", "enc", "f1", "f2", "label", "pair")
dat[["id"]] <- paste0("Encoding a, subgroup ", dat[["id"]])


ggplot(dat, aes(x = f1, y = f2, colour = pair, label = label)) +
  geom_line() +
  geom_point(aes(x = f1, y = f2, colour = enc), size = 4) + 
  facet_wrap(~ id) + 
  geom_text(aes(x = f1, y = f2, colour = enc, label = label), vjust = 1, size = 5) + 
  scale_color_brewer(palette="Dark2", guide = "none") +
  my_theme
```

The figure above represents the distances between groups of encoding **a** (green dots) and groups of encoding **b** (red dots). The position of the dot defined by mean values of specific properties of all amino acids belonging to the group.

```{r, echo = FALSE, message = FALSE, results='asis'}
tmp <- sapply(coords_a, function(single_coords_a) {
  distances <- sapply(coords_b, function(single_coords_b) 
    #vector of distances between groups
    sqrt(sum((single_coords_a - single_coords_b)^2))
  )
  #c(dist = min(distances), id = unname(which.min(distances)))
  distances
})

colnames(tmp) <- paste0("Enc a, group ", colnames(tmp))
rownames(tmp) <- paste0("Enc b, group ", rownames(tmp))

kable(tmp, caption = "Distances between groups of encodings a and b.")
```

For each group in encoding **a**, we choose the minimum distance $d_i$ between the group and any group belonging to the encoding **b**. The normalization factor is equal to the sum of minimum distances for each group in **a**.

$$
n_f = \sum_{i \in 1, 2, 3} \min \left( d_i \right) 
$$

Problematic situations:  
1. Unequal number of groups.  
2. Source of properties to calculate mean properties of groups (default: properties used in clusterization to create the eoncoding).  

# Cross-validation

Amino acid encodings used in the cross-validation: all 18 537 reduced alphabets created by clusterization, two common reduced alphabets, raw amino acid sequence.  

Previously we trained classifiers using three distinct data sets containing sequences of length 6 and shorter than 11 and 16 amino acids. The smaller data sets were subsets of larger data sets.  

Possible new schemes:  
1. Positive training data sets: only sequences on length 6. Negative training set: sequences shorter than 16 amino acids.
2. All combinations of positive data sets (6, 11, 15) and negative data set (6, 11, 15).